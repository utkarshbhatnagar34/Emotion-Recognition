{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Emotion Recognition(Model Training).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCjB2HtSc2j1",
        "outputId": "8dadc09f-ad91-47b4-a4fa-82642bdfe0cf"
      },
      "source": [
        "#to unzip the file upload on the colab\n",
        "from zipfile import ZipFile\n",
        "file_name='/content/archive (1).zip'\n",
        "\n",
        "with ZipFile(file_name,'r') as zip:\n",
        "  zip.extractall()\n",
        "  print('Done')\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rA610t1iPNb",
        "outputId": "4eee0743-d009-45dd-e5e1-e1083ba9d6ca"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "#pointer to data\n",
        "train_dir = '/content/train'\n",
        "val_dir = '/content/test'\n",
        "\n",
        "#rescaling data so pixel lie between 0 to 1(to reduce useless data) \n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "#it add pointer to the data  and divide the data into batches(of size 64)\n",
        "#we use batches to calculate the gradient decent and move in that direction(help train the more efficiently)\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(48,48),\n",
        "        batch_size=64,\n",
        "        color_mode=\"grayscale\",\n",
        "        class_mode='categorical')\n",
        "\n",
        "#test data\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "        val_dir,\n",
        "        target_size=(48,48),\n",
        "        batch_size=64,\n",
        "        color_mode=\"grayscale\",\n",
        "        class_mode='categorical')\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "#sequential means that our layer are added in linear fashion(input)\n",
        "#A Sequential model is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor\n",
        "from keras.models import Sequential\n",
        "#\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import MaxPooling2D\n",
        "\n",
        "emotion_model = Sequential()\n",
        "#kernel is size of filter we apply on the data \n",
        "#we use relu function as mathematical function to change the out form linear to non linear model\n",
        "#input shape is 3D \n",
        "emotion_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48,48,1)))\n",
        "emotion_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "#pooling layer help to remove excess information by applying a kernel=(3,3) to the image pixel \n",
        "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "#it dropout some randomly selected neuron (0.25)=25% of neuron are droppped out(it help to genearlise the model)\n",
        "emotion_model.add(Dropout(0.25))\n",
        "\n",
        "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
        "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
        "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "emotion_model.add(Dropout(0.25))\n",
        "\n",
        "# as input shape is 3D we require output in 1D(single value) so we need to flatten the layer\n",
        "emotion_model.add(Flatten())\n",
        "emotion_model.add(Dense(1024, activation='relu'))\n",
        "emotion_model.add(Dropout(0.5))\n",
        "emotion_model.add(Dense(7, activation='softmax'))\n",
        "\n",
        "#compile-all layer of model are compile and ready to use\n",
        "#metric=accuracy means that accuracy is the main parameter on which the answer(output) is decided\n",
        "emotion_model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.0001, decay=1e-6),metrics=['accuracy'])\n",
        "\n",
        "#now training the model\n",
        "emotion_model_info = emotion_model.fit_generator(\n",
        "        train_generator,\n",
        "        steps_per_epoch=28709 // 64,\n",
        "        epochs=50,\n",
        "        validation_data=validation_generator,\n",
        "        validation_steps=7178 // 64)\n",
        "\n",
        "emotion_model.save_weights('Emotion_recogniton_model.h5')\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 28709 images belonging to 7 classes.\n",
            "Found 7178 images belonging to 7 classes.\n",
            "WARNING:tensorflow:From <ipython-input-5-db10999f9bb6>:72: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "Epoch 1/50\n",
            "448/448 [==============================] - 10s 22ms/step - loss: 1.7979 - accuracy: 0.2644 - val_loss: 1.6948 - val_accuracy: 0.3348\n",
            "Epoch 2/50\n",
            "448/448 [==============================] - 10s 22ms/step - loss: 1.6234 - accuracy: 0.3675 - val_loss: 1.5476 - val_accuracy: 0.4079\n",
            "Epoch 3/50\n",
            "448/448 [==============================] - 10s 21ms/step - loss: 1.5287 - accuracy: 0.4110 - val_loss: 1.4856 - val_accuracy: 0.4237\n",
            "Epoch 4/50\n",
            "448/448 [==============================] - 9s 21ms/step - loss: 1.4585 - accuracy: 0.4400 - val_loss: 1.4025 - val_accuracy: 0.4688\n",
            "Epoch 5/50\n",
            "448/448 [==============================] - 10s 22ms/step - loss: 1.3990 - accuracy: 0.4667 - val_loss: 1.3538 - val_accuracy: 0.4881\n",
            "Epoch 6/50\n",
            "448/448 [==============================] - 10s 21ms/step - loss: 1.3542 - accuracy: 0.4870 - val_loss: 1.3158 - val_accuracy: 0.5088\n",
            "Epoch 7/50\n",
            "448/448 [==============================] - 10s 22ms/step - loss: 1.3104 - accuracy: 0.5056 - val_loss: 1.2848 - val_accuracy: 0.5116\n",
            "Epoch 8/50\n",
            "448/448 [==============================] - 9s 21ms/step - loss: 1.2715 - accuracy: 0.5198 - val_loss: 1.2560 - val_accuracy: 0.5254\n",
            "Epoch 9/50\n",
            "448/448 [==============================] - 10s 21ms/step - loss: 1.2356 - accuracy: 0.5345 - val_loss: 1.2250 - val_accuracy: 0.5419\n",
            "Epoch 10/50\n",
            "448/448 [==============================] - 10s 21ms/step - loss: 1.2056 - accuracy: 0.5483 - val_loss: 1.2258 - val_accuracy: 0.5357\n",
            "Epoch 11/50\n",
            "448/448 [==============================] - 10s 22ms/step - loss: 1.1738 - accuracy: 0.5593 - val_loss: 1.1995 - val_accuracy: 0.5448\n",
            "Epoch 12/50\n",
            "448/448 [==============================] - 10s 22ms/step - loss: 1.1451 - accuracy: 0.5739 - val_loss: 1.1685 - val_accuracy: 0.5564\n",
            "Epoch 13/50\n",
            "448/448 [==============================] - 10s 21ms/step - loss: 1.1198 - accuracy: 0.5833 - val_loss: 1.1591 - val_accuracy: 0.5611\n",
            "Epoch 14/50\n",
            "448/448 [==============================] - 10s 21ms/step - loss: 1.0967 - accuracy: 0.5898 - val_loss: 1.1495 - val_accuracy: 0.5647\n",
            "Epoch 15/50\n",
            "448/448 [==============================] - 10s 21ms/step - loss: 1.0729 - accuracy: 0.5994 - val_loss: 1.1428 - val_accuracy: 0.5686\n",
            "Epoch 16/50\n",
            "448/448 [==============================] - 10s 21ms/step - loss: 1.0456 - accuracy: 0.6091 - val_loss: 1.1261 - val_accuracy: 0.5756\n",
            "Epoch 17/50\n",
            "448/448 [==============================] - 9s 21ms/step - loss: 1.0163 - accuracy: 0.6207 - val_loss: 1.1224 - val_accuracy: 0.5838\n",
            "Epoch 18/50\n",
            "448/448 [==============================] - 9s 21ms/step - loss: 0.9968 - accuracy: 0.6291 - val_loss: 1.1196 - val_accuracy: 0.5822\n",
            "Epoch 19/50\n",
            "448/448 [==============================] - 10s 21ms/step - loss: 0.9745 - accuracy: 0.6389 - val_loss: 1.1082 - val_accuracy: 0.5815\n",
            "Epoch 20/50\n",
            "448/448 [==============================] - 10s 21ms/step - loss: 0.9491 - accuracy: 0.6479 - val_loss: 1.1036 - val_accuracy: 0.5862\n",
            "Epoch 21/50\n",
            "448/448 [==============================] - 10s 21ms/step - loss: 0.9276 - accuracy: 0.6544 - val_loss: 1.1069 - val_accuracy: 0.5914\n",
            "Epoch 22/50\n",
            "448/448 [==============================] - 10s 21ms/step - loss: 0.9063 - accuracy: 0.6684 - val_loss: 1.0938 - val_accuracy: 0.5936\n",
            "Epoch 23/50\n",
            "448/448 [==============================] - 9s 21ms/step - loss: 0.8840 - accuracy: 0.6736 - val_loss: 1.0931 - val_accuracy: 0.5957\n",
            "Epoch 24/50\n",
            "448/448 [==============================] - 10s 21ms/step - loss: 0.8547 - accuracy: 0.6885 - val_loss: 1.0933 - val_accuracy: 0.6011\n",
            "Epoch 25/50\n",
            "448/448 [==============================] - 10s 21ms/step - loss: 0.8324 - accuracy: 0.6955 - val_loss: 1.0935 - val_accuracy: 0.5956\n",
            "Epoch 26/50\n",
            "448/448 [==============================] - 10s 21ms/step - loss: 0.8123 - accuracy: 0.7025 - val_loss: 1.0884 - val_accuracy: 0.6062\n",
            "Epoch 27/50\n",
            "448/448 [==============================] - 10s 21ms/step - loss: 0.7827 - accuracy: 0.7135 - val_loss: 1.1016 - val_accuracy: 0.6028\n",
            "Epoch 28/50\n",
            "448/448 [==============================] - 9s 21ms/step - loss: 0.7655 - accuracy: 0.7187 - val_loss: 1.0871 - val_accuracy: 0.6060\n",
            "Epoch 29/50\n",
            "448/448 [==============================] - 10s 21ms/step - loss: 0.7355 - accuracy: 0.7333 - val_loss: 1.0925 - val_accuracy: 0.6074\n",
            "Epoch 30/50\n",
            "448/448 [==============================] - 10s 22ms/step - loss: 0.7166 - accuracy: 0.7366 - val_loss: 1.1049 - val_accuracy: 0.6044\n",
            "Epoch 31/50\n",
            "448/448 [==============================] - 9s 21ms/step - loss: 0.6955 - accuracy: 0.7464 - val_loss: 1.0962 - val_accuracy: 0.6045\n",
            "Epoch 32/50\n",
            "448/448 [==============================] - 10s 21ms/step - loss: 0.6752 - accuracy: 0.7566 - val_loss: 1.0984 - val_accuracy: 0.6133\n",
            "Epoch 33/50\n",
            "448/448 [==============================] - 9s 21ms/step - loss: 0.6595 - accuracy: 0.7614 - val_loss: 1.0994 - val_accuracy: 0.6133\n",
            "Epoch 34/50\n",
            "448/448 [==============================] - 10s 21ms/step - loss: 0.6383 - accuracy: 0.7705 - val_loss: 1.1087 - val_accuracy: 0.6155\n",
            "Epoch 35/50\n",
            "448/448 [==============================] - 9s 21ms/step - loss: 0.6208 - accuracy: 0.7730 - val_loss: 1.1042 - val_accuracy: 0.6124\n",
            "Epoch 36/50\n",
            "448/448 [==============================] - 9s 21ms/step - loss: 0.5991 - accuracy: 0.7858 - val_loss: 1.1177 - val_accuracy: 0.6170\n",
            "Epoch 37/50\n",
            "448/448 [==============================] - 10s 21ms/step - loss: 0.5809 - accuracy: 0.7903 - val_loss: 1.1230 - val_accuracy: 0.6222\n",
            "Epoch 38/50\n",
            "448/448 [==============================] - 10s 22ms/step - loss: 0.5559 - accuracy: 0.7976 - val_loss: 1.1283 - val_accuracy: 0.6116\n",
            "Epoch 39/50\n",
            "448/448 [==============================] - 10s 21ms/step - loss: 0.5350 - accuracy: 0.8075 - val_loss: 1.1392 - val_accuracy: 0.6214\n",
            "Epoch 40/50\n",
            "448/448 [==============================] - 9s 21ms/step - loss: 0.5246 - accuracy: 0.8097 - val_loss: 1.1390 - val_accuracy: 0.6217\n",
            "Epoch 41/50\n",
            "448/448 [==============================] - 10s 21ms/step - loss: 0.5013 - accuracy: 0.8194 - val_loss: 1.1653 - val_accuracy: 0.6197\n",
            "Epoch 42/50\n",
            "448/448 [==============================] - 10s 21ms/step - loss: 0.4927 - accuracy: 0.8192 - val_loss: 1.1539 - val_accuracy: 0.6201\n",
            "Epoch 43/50\n",
            "448/448 [==============================] - 10s 21ms/step - loss: 0.4673 - accuracy: 0.8323 - val_loss: 1.1578 - val_accuracy: 0.6200\n",
            "Epoch 44/50\n",
            "448/448 [==============================] - 10s 22ms/step - loss: 0.4587 - accuracy: 0.8353 - val_loss: 1.1828 - val_accuracy: 0.6221\n",
            "Epoch 45/50\n",
            "448/448 [==============================] - 10s 22ms/step - loss: 0.4456 - accuracy: 0.8382 - val_loss: 1.1833 - val_accuracy: 0.6217\n",
            "Epoch 46/50\n",
            "448/448 [==============================] - 10s 21ms/step - loss: 0.4307 - accuracy: 0.8441 - val_loss: 1.1757 - val_accuracy: 0.6203\n",
            "Epoch 47/50\n",
            "448/448 [==============================] - 10s 21ms/step - loss: 0.4147 - accuracy: 0.8475 - val_loss: 1.1937 - val_accuracy: 0.6222\n",
            "Epoch 48/50\n",
            "448/448 [==============================] - 9s 21ms/step - loss: 0.4094 - accuracy: 0.8546 - val_loss: 1.1803 - val_accuracy: 0.6229\n",
            "Epoch 49/50\n",
            "448/448 [==============================] - 10s 21ms/step - loss: 0.3992 - accuracy: 0.8557 - val_loss: 1.1999 - val_accuracy: 0.6226\n",
            "Epoch 50/50\n",
            "448/448 [==============================] - 10s 21ms/step - loss: 0.3800 - accuracy: 0.8627 - val_loss: 1.2089 - val_accuracy: 0.6214\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5FdHI6KlGOd"
      },
      "source": [
        "from keras.models import load_model\n",
        "nw=load_model('/content/Emotion_recogniton_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFBMJ4yKmcdo"
      },
      "source": [
        "emotion_model.save('Emotion_recogniton.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAw3xUPXnBd4"
      },
      "source": [
        "from keras.models import load_model\n",
        "nw=load_model('Emotion_recogniton.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35Vj1VUfnS1B"
      },
      "source": [
        "/content/Emotion_recogniton.h5"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}